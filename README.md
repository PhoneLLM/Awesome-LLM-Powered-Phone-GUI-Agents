# LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
[![GitHub stars](https://img.shields.io/github/stars/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents)](https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents)](https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents/network)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents)
[![GitHub issues](https://img.shields.io/github/issues/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents)](https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents/issues)
[![GitHub pulls](https://img.shields.io/github/issues-pr/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents)](https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents/pulls)
[![Contributors](https://img.shields.io/github/contributors/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents)](https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents/graphs/contributors)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

## üöÄ News

- üí• [2025/01/07] Our survey is released! See [LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects](https://www.preprints.org/manuscript/202501.0413/v1) for the paper!
- ‚ú® [2024/12/29] We create this repository to maintain a paper list on LLM-Powered Phone GUI Agents. More papers are coming soon!

## üìë Citation
If you find our survey useful for your research and applications, please cite using this BibTeX:
```
@article{liu2025llm,
  title={Llm-powered gui agents in phone automation: Surveying progress and prospects},
  author={Liu, William and Liu, Liang and Guo, Yaxuan and Xiao, Han and Lin, Weifeng and Chai, Yuxiang and Ren, Shuai and Liang, Xiaoyu and Li, Linghao and Wang, Wenhao and others},
  year={2025},
  publisher={Preprints}
}
```
## üìñ Introduction

üî• **Must-read papers for LLM-Powered Phone GUI Agents.**

**We greatly appreciate any contributions via PRs, issues, emails, or other methods.**

## üîñ General Overview
A comprehensive taxonomy of LLM-powered phone GUI agents in phone automation. 
Note that only a selection of representative works is included in this categorization.

![overviews.png](figs/overviews.png)

## ü™ß Milestones
Milestones in the development of LLM-powered phone GUI agents. 
This figure divides advancements into four primary parts: 
**Prompt Engineering**, **Training-Based Methods**, **Datasets** and **Benchmarks**. 
Prompt Engineering leverages pre-trained LLMs by strategically crafting input prompts, to perform specific tasks without modifying model parameters. 
In contrast, Training-Based Methods, involve adapting LLMs via supervised fine-tuning or reinforcement learning on GUI-specific data, 
thereby enhancing their ability to understand and interact with mobile UIs.

![milestones.png](figs/milestones.png)

## üìù Table of Content (ToC)

- [LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects](#llm-powered-ui-agents-in-phone-automation-surveying-progress-and-prospects)
  - [üöÄ News](#-news)
  - [üìñ Introduction](#-introduction)
  - [üîñ General Overview](#-general-overview)
  - [ü™ß Milestones](#-milestones)
  - [üìù Table of Content (ToC)](#-table-of-content-toc)
  - [üîç Paper List](#-paper-list)
  - [üåü Star History](#-star-history)

## üîç Paper List

| ID   | Date    | Method                                                       | Stars                                                        |
| ---- | ------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1    | 2023.04 | [DroidBot-GPT: GPT-powered UI Automation for Android](https://arxiv.org/abs/2304.07061) | /                                                            |
| 2    | 2023.06 | [Mind2Web: Towards a Generalist Agent for the Web](https://arxiv.org/abs/2306.06070) | ![Stars](https://img.shields.io/github/stars/OSU-NLP-Group/Mind2Web) |
| 3    | 2023.09 | [You Only Look at Screens: Multimodal Chain-of-Action Agents](https://arxiv.org/abs/2309.11436) | ![Stars](https://img.shields.io/github/stars/cooelf/Auto-GUI) |
| 4    | 2023.09 | [AutoDroid: LLM-powered Task Automation in Android](https://arxiv.org/abs/2308.15272) | ![Stars](https://img.shields.io/github/stars/MobileLLM/AutoDroid) |
| 5    | 2023.10 | [SteP: Stacked LLM Policies for Web Actions](https://arxiv.org/abs/2310.03720) | ![Stars](https://img.shields.io/github/stars/asappresearch/webagents-step) |
| 6    | 2023.11 | [ GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation](https://arxiv.org/abs/2311.07562) | ![Stars](https://img.shields.io/github/stars/zzxslp/MM-Navigator) |
| 7    | 2023.12 | [CogAgent: A Visual Language Model for GUI Agents](https://arxiv.org/abs/2312.08914) | ![Stars](https://img.shields.io/github/stars/THUDM/CogVLM)   |
| 8    | 2023.12 | [ AppAgent: Multimodal Agents as Smartphone Users](https://arxiv.org/abs/2312.13771) | ![Stars](https://img.shields.io/github/stars/mnotgod96/AppAgent) |
| 9    | 2023.12 | [VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning](https://arxiv.org/abs/2312.11190v2) | ![Stars](https://img.shields.io/github/stars/AkimotoAyako/VisionTasker) |
| 10   | 2023.12 | [Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation](https://arxiv.org/abs/2312.03003) | ![Stars](https://img.shields.io/github/stars/mobilegptsys/MobileGPT) |
| 11   | 2023.12 | [Dual-View Visual Contextualization for Web Navigation](https://arxiv.org/abs/2402.04476) | /                                                            |
| 12   | 2023.12 | [ WebVLN: Vision-and-Language Navigation on Websites](https://arxiv.org/abs/2312.15820) | ![Stars](https://img.shields.io/github/stars/WebVLN/WebVLN)  |
| 13   | 2024.01 | [Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception](https://arxiv.org/abs/2401.16158) | ![Stars](https://img.shields.io/github/stars/X-PLUG/MobileAgent) |
| 14   | 2024.01 | [MobileAgent: enhancing mobile control via human-machine interaction and SOP integration](https://arxiv.org/abs/2401.04124) | ![Stars](https://img.shields.io/github/stars/alipay/mobile-agent) |
| 15   | 2024.01 | [SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents](https://arxiv.org/abs/2401.10935) | ![Stars](https://img.shields.io/github/stars/njucckevin/SeeClick) |
| 16   | 2024.01 | [GPT-4V(ision) is a Generalist Web Agent, if Grounded](https://arxiv.org/abs/2401.01614) | ![Stars](https://img.shields.io/github/stars/OSU-NLP-Group/SeeAct) |
| 17   | 2024.02 | [ScreenAgent: A Vision Language Model-driven Computer Control Agent](https://arxiv.org/abs/2402.07945) | ![Stars](https://img.shields.io/github/stars/niuzaisheng/ScreenAgent) |
| 18   | 2024.02 | [UFO: A UI-Focused Agent for Windows OS Interaction](https://arxiv.org/abs/2402.07939) | ![Stars](https://img.shields.io/github/stars/microsoft/UFO)  |
| 19   | 2024.02 | [ScreenAI: A Vision-Language Model for UI and Infographics Understanding](https://arxiv.org/abs/2402.04615) | ![Stars](https://img.shields.io/github/stars/google-research-datasets/screen_qa) |
| 20   | 2024.02 | [WebLINX: Real-World Website Navigation with Multi-Turn Dialogue](https://arxiv.org/abs/2402.05930) | ![Stars](https://img.shields.io/github/stars/McGill-NLP/WebLINX) |
| 21   | 2024.02 | [AndroidArena: Understanding the Weakness of Large Language Model Agents within a Complex Android Environment](https://dl.acm.org/doi/abs/10.1145/3637528.3671650) | ![Stars](https://img.shields.io/github/stars/AndroidArenaAgent/AndroidArena) |
| 22   | 2024.02 | [CoCo-agent: A comprehensive cognitive MLLM agent for smartphone GUI automation](https://arxiv.org/abs/2402.11941) | ![Stars](https://img.shields.io/github/stars/xbmxb/CoCo-Agent) |
| 23   | 2024.03 | [Android in the Zoo: Chain-of-Action-Thought for GUI Agents](https://arxiv.org/abs/2403.02713) | ![Stars](https://img.shields.io/github/stars/IMNearth/CoAT)  |
| 24   | 2024.03 | [Trial and error Exploration-based trajectory optimization for LLM agents](https://arxiv.org/abs/2403.02502) | ![Stars](https://img.shields.io/github/stars/Yifan-Song793/ETO) |
| 25   | 2024.04 | [Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs](https://arxiv.org/abs/2404.05719) | /                                                            |
| 26   | 2024.04 | [Octopus v2: On-device language model for super agent](https://arxiv.org/abs/2404.01744) | /                                                            |
| 27   | 2024.04 | [AutoWebGLM: A Large Language Model-based Web Navigating Agent](https://arxiv.org/abs/2404.03648) | ![Stars](https://img.shields.io/github/stars/THUDM/AutoWebGLM) |
| 28   | 2024.04 | [ ReALM: Reference Resolution As Language Modeling](https://arxiv.org/abs/2403.20329) | /                                                            |
| 29   | 2024.04 | [LlamaTouch: A Faithful and Scalable Testbed for Mobile UI Task Automation](https://arxiv.org/abs/2404.16054) | ![Stars](https://img.shields.io/github/stars/llamatouch/llamatouch) |
| 30   | 2024.04 | [ MMAC-Copilot: Multi-modal Agent Collaboration Operating System Copilot](https://arxiv.org/abs/2404.18074) | /                                                            |
| 31   | 2024.04 | [Search Beyond Queries: Training Smaller Language Models for Web Interactions via Reinforcement Learning](https://arxiv.org/abs/2404.10887) | /                                                            |
| 32   | 2024.06 | [Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration](https://arxiv.org/abs/2406.01014) | ![Stars](https://img.shields.io/github/stars/X-PLUG/MobileAgent) |
| 33   | 2024.06 | [DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning](https://arxiv.org/abs/2406.11896) | ![Stars](https://img.shields.io/github/stars/DigiRL-agent/digirl) |
| 34   | 2024.06 | [OS-Copilot: Towards Generalist Computer Agents with Self-Improvement](https://arxiv.org/abs/2402.07456) | ![Stars](https://img.shields.io/github/stars/OS-Copilot/OS-Copilot) |
| 35   | 2024.06 | [Do multimodal foundation models understand enterprise workflows? A benchmark for business process management tasks](https://ui.adsabs.harvard.edu/abs/2024arXiv240613264W/abstract) | ![Stars](https://img.shields.io/github/stars/HazyResearch/wonderbread) |
| 36   | 2024.06 | [ GUICourse: From General Vision Language Models to Versatile GUI Agents](https://arxiv.org/abs/2406.11317) | ![Stars](https://img.shields.io/github/stars/yiye3/GUICourse) |
| 37   | 2024.06 | [GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices](https://arxiv.org/abs/2406.08451) | ![Stars](https://img.shields.io/github/stars/OpenGVLab/GUI-Odyssey) |
| 38   | 2024.06 | [ MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents](https://arxiv.org/abs/2406.08184) | ![Stars](https://img.shields.io/github/stars/MobileAgentBench/mobile-agent-bench) |
| 39   | 2024.07 | [ E-ANT: A Large-Scale Dataset for Efficient Automatic GUI NavigaTion](https://arxiv.org/abs/2406.14250) | /                                                            |
| 40   | 2024.07 | [Enhancing Mobile ""How-to"" Queries with Automated Search Results Verification and Reranking](https://openreview.net/forum?id=EaSKCja3rr) | /                                                            |
| 41   | 2024.07 | [MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices](https://arxiv.org/abs/2407.03913) | /                                                            |
| 42   | 2024.07 | [Cradle: Empowering Foundation Agents Towards General Computer Control](https://arxiv.org/abs/2403.03186) | ![Stars](https://img.shields.io/github/stars/BAAI-Agents/Cradle) |
| 43   | 2024.07 | [On the Effects of Data Scale on Computer Control Agents](https://arxiv.org/abs/2406.03679) | [![Code](https://img.shields.io/badge/Code-025E8C?style=for-the-badge)](https://github.com/google-research/google-research/tree/master/android_control) |
| 44   | 2024.07 | [Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents](https://arxiv.org/abs/2407.00993) | /                                                            |
| 45   | 2024.07 | [AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents](https://arxiv.org/abs/2407.17490) | ![Stars](https://img.shields.io/github/stars/YuxiangChai/AMEX-codebase) |
| 46   | 2024.07 | [Agent-E: From Autonomous Web Navigation to Foundational Design Principles in Agentic Systems](https://arxiv.org/abs/2407.13032) | /                                                            |
| 47   | 2024.07 | [Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?](https://arxiv.org/abs/2407.10956) | ![Stars](https://img.shields.io/github/stars/xlang-ai/Spider2-V) |
| 48   | 2024.07 | [ Security Matrix for Multimodal Agents on Mobile Devices: A Systematic and Proof of Concept Study](https://arxiv.org/abs/2407.09295) | /                                                            |
| 49   | 2024.07 | [ AUITestAgent: Automatic Requirements Oriented GUI Function Testing](https://arxiv.org/abs/2407.09018) | ![Stars](https://img.shields.io/github/stars/bz-lab/AUITestAgent) |
| 50   | 2024.07 | [ MobileFlow: A Multimodal LLM For Mobile GUI Agent](https://arxiv.org/abs/2407.04346) | /                                                            |
| 51   | 2024.07 | [Seeing is Believing: Vision-driven Non-crash Functional Bug Detection for Mobile Apps](https://arxiv.org/abs/2407.03037) | ![Stars](https://img.shields.io/github/stars/testtestA6/VisionDroid) |
| 52   | 2024.08 | [Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents](https://arxiv.org/abs/2408.07199) | /                                                            |
| 53   | 2024.08 | muAgent: An Innovative Agent Framework Driven by KG Engine   | ![Stars](https://img.shields.io/github/stars/codefuse-ai/CodeFuse-muAgent) |
| 54   | 2024.08 | [Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions](https://arxiv.org/abs/2408.02544) | ![Stars](https://img.shields.io/github/stars/xbmxb/EnvDistraction) |
| 55   | 2024.08 | [OmniParser for Pure Vision Based GUI Agent](https://arxiv.org/abs/2408.00203) | /                                                            |
| 56   | 2024.08 | [OpenWebAgent: An open toolkit to enable web agents on large language models](https://aclanthology.org/2024.acl-demos.8/) | ![Stars](https://img.shields.io/github/stars/THUDM/OpenWebAgent) |

## üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents&type=Date)](https://star-history.com/#PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents&Date)
